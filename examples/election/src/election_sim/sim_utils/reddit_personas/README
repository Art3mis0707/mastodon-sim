# Part (A) : Getting Reddit data from the r/politics subReddit

 

# Part (B) : Ranking posts based on interaction

The code conducts a statistical post-processing of a dataset of social media submissions. Starting from an input JSON file representing multiple posts, it extracts three primary variables for each entry: the `aggregate score` (a measure of popularity or user appraisal), the `upvote ratio` (proportion of positive votes out of total votes), and the `number of comments`. It uses these raw metrics to derive two derived parameters:

1. **Number of Votes**: Estimated using a formula that relates the post’s total score to its upvote ratio. This calculation is grounded in an inverse mapping from the combined attributes of a post’s score and the fraction of users who positively engaged with it. Mathematically, it assumes the relationship:
    
    `*Number of Votes = Score / (2 × Upvote Ratio – 1)*`
    
    This formula treats the score and upvote ratio as correlates of an underlying voting process, translating the observed aggregated metrics into an approximate count of total votes cast.
    
2. **Engagement Ratio**: Computed as the ratio of the number of comments to the inferred number of votes. The engagement ratio quantifies how actively the post’s audience participated through discussion relative to their voting behavior. It effectively normalizes the volume of discourse by the base level of attention or approval that a post received:
    
    `*Engagement Ratio = Number of Comments / Number of Votes*`
    

The code filters out any posts for which these derived calculations are not feasible. After producing a clean dataset of valid posts, it sorts them in descending order by their computed engagement ratio. This sorted dataset is then written to a new JSON file, enabling subsequent statistical analyses or comparisons of posts based on their relative levels of audience interaction.

# Part (B.1) : Extracting the top-k posts

This code loads a JSON dataset and identifies a subset of the most prominent records based on a specified numeric metric. It first deserializes a JSON array of data objects from a file. Assuming all objects share a common attribute defined by `key`, it applies a sorting operation to reorder the entire dataset in descending order according to the values of that attribute. Subsequently, it selects the top `k` objects from this sorted sequence. These top `k` objects are then serialized and written out to a new JSON file.

Part (B.2) : Extracting top users’ usernames

It reads from an input JSON file contains the top-k posts. From each record, it extracts the author’s name (identified by the `author_name` field) and collates these names into a single list.

Once extracted, these names are written out line-by-line into a plain text file

# Part (C) : Getting Reddit users’ data

# Part (D) : Cleaning the Reddit Data

This script performs a data transformation task on a collection of JSON objects representing posts. Initially, each post is encoded as an independent record that associates an `author_id` with a corresponding `title`. 

It parses the input JSON file containing the raw scraped data, iterates through the included posts, and classifies them according to their `author_id`. During this classification, a new data structure is built such that each `author_id` serves as a key, and its value is a consolidated list of all `titles` associated with that author. After this grouping operation, the code outputs a new JSON file where the data is organized on a per-author basis.

# Part (E) : Persona Generation with the LLM

This code operationalizes a methodology for generating "personas"—from our dataset of users’ Reddit activity. 

1. **Input Data and Structure**:
    
    The cleaned data of individual users’s activity is pass as input. It is of the form 
    
    `{`
    
    `“author_id” :`
    
    `“titles” : [`
    
    `]`
    
    `}`
    
2. **Prompts**
    
    The code uses two primary prompt components sent to the OpenAI model: a *system message* and a *user message*. These components work together to instruct the model on how to transform the input data (user interactions) into persona profiles.
    
    1. **System Message**:
        - **Content**:
            
            *"You're a psychologist working in social and political sciences. You have to extract information about people based on interactions they have with others, and use it to design personas for a simulation."*
            
        - **Purpose**:
            
            This message sets the overall role and context for the model. It frames the model as a social and political science psychologist whose job is to analyze user interactions and create corresponding persona profiles. By doing this, it ensures that the model’s responses will be aligned with the intended disciplinary perspective and objectives.
            
    2. **User Message** (per batch):
        
        The user message provides the data to analyze and the instructions for the output. It is constructed as follows:
        
        - **Batch Data**:
            
            The message begins by including a JSON-serialized batch of user interaction data. This is done by converting a subset of the input dataset (`batch_data`) into JSON format. The data presumably contains entries that represent user-generated posts, comments, or other interaction records. This section of the prompt provides the empirical evidence (the raw data) upon which the model will base its persona generation.
            
        - **Instructions for Persona Creation**:
            
            After the raw data, the prompt includes a detailed persona descriptor template. This template specifies the attributes and fields that each persona should include. It instructs the model to produce:
            
            - **Basic Identifying Information**: Name, a user reference (indicating which user the persona is based on), and gender/sex.
            - **Political Identity**: A descriptor of the individual’s political orientation.
            - **Personality Traits (Big 5)**: Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism (each rated on a scale from 1 to 10).
            - **Schwartz Values**: A set of core motivational values (e.g., Self-Direction, Benevolence, Power) each rated on a scale from 1 to 10.
            - **Description Based on Interactions**: A qualitative summary that captures the user’s character, behavior, or attitudes inferred from the interaction data.
            
            The template also includes specific instructions:
            
            - **Realistic Scoring**: Assign ratings thoughtfully and consistently.
            - **Internal Consistency**: Ensure that trait scores and values logically match the observed interactions.
            - **Realistic Naming**: Assign believable names to the personas.
        - **Output Format Restrictions**:
            
            Importantly, the prompt ends with a direct instruction: *"IMPORTANT: Return ONLY the JSON objects for each person, nothing else. No additional text, explanations, or formatting. Only the JSON."*
            
            This directive ensures that the model’s output is strictly limited to the requested JSON persona objects, without additional commentary or formatting. It helps maintain a clean, machine-readable output.
            
3. **Batch Processing and API Integration**:
    
    To manage computational load and adhere to API constraints, the script processes the data in batches. It sends each batch—consisting of multiple user interaction records—along with the structured prompt to the OpenAI API. The returned output from the model is directly captured
    
4. **Output**:
    
    The final result is a series of JSON-formatted persona objects. Each object comprises a "Name," a "User_Reference," and calibrated scores or descriptors for Political Identity, the Big Five traits, and the Schwartz values. Additionally, a free-text “Description based on interactions” field provides  qualitative information. 
    
    # Part (F) : Post-processing cleaning
    
    - **Reading the Raw Text**:
        
        The script begins by reading the entirety of a text file (`persona.txt`). This file contains multiple persona JSON objects, possibly intermixed with extra formatting or markup intended for human readability.
        
    - **Removing Unnecessary Markup**:
        
        In many AI-generated contexts, code blocks or formatting signals such as triple backticks (`) and language-specific hints (e.g.,` json) may appear. These are artifacts that need to be removed to obtain clean JSON data. 
        
    - **Ensuring Valid JSON Structure**:
        
        Because the original data may have been generated in pieces or concatenated, the script tries to reconstruct a proper JSON array. It ensures that the final string starts with `[` and ends with `]`, forming a well-defined list. It also replaces occurrences of improperly concatenated objects (e.g., `}{` without commas) with well-formed JSON syntax such as `},\n{`.
        
        Regular expressions (`re`) are employed to detect and correct structural irregularities:
        
        - `"}\s*{"` patterns are replaced with `},\n{` to separate adjacent objects with a comma.
        - Unnecessary extra commas are removed (e.g., `", , "` becomes `","`).
        - Trailing commas before closing brackets are eliminated for correct JSON formatting.
    - **Error Handling and Validation**:
        
        After making these adjustments, the script attempts to parse the resulting text using `json.loads()`. If this step succeeds, it confirms that the raw text has been successfully transformed into a valid JSON structure. Any failure to parse raises a `JSONDecodeError`, and the script prints diagnostic information to help locate and fix the formatting issues.
        
    - **Saving the Cleaned Data**:
        
        If the data is successfully parsed, it is then written to a new file (`cleaned_personas.json`) in a standardized JSON format.